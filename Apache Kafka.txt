#What is Apache Kafka?
 Apache Kafka is a distributed data store optimized for ingesting and processing streaming data in
real-time. Streaming data is data that is continuously generated by thousands of data sources,
which typically send the data records in simultaneously. A streaming platform needs to handle
this constant influx of data, and process the data sequentially and incrementally.

Kafka provides three main functions to its users:

Publish and subscribe to streams of records
Effectively store streams of records in the order in which records were generated
Process streams of records in real time
Kafka is primarily used to build real-time streaming data pipelines and applications that adapt to
the data streams. It combines messaging, storage, and stream processing to allow storage and analysis
of both historical and real-time data.  


#Why would you use Kafka?
  Kafka is used to build real-time streaming data pipelines and real-time streaming applications. A data
pipeline reliably processes and moves data from one system to another, and a streaming application is
an application that consumes streams of data. For example, if you want to create a data pipeline that
takes in user activity data to track how people use your website in real-time, Kafka would be used to
ingest and store streaming data while serving reads for the applications powering the data pipeline.
Kafka is also often used as a message broker solution, which is a platform that processes and mediates
communication between two applications.

#How does Kafka work?
  Kafka combines two messaging models, queuing and publish-subscribe, to provide the key benefits of
each to consumers. Queuing allows for data processing to be distributed across many consumer instances,
making it highly scalable. However, traditional queues aren’t multi-subscriber.

#Benefits of Kafka's approach
1)Scalable
Kafka’s partitioned log model allows data to be distributed across multiple servers, making it scalable
beyond what would fit on a single server. 
2)Fast
Kafka decouples data streams so there is very low latency, making it extremely fast. 
3)Durable
Partitions are distributed and replicated across many servers, and the data is all written to disk.
This helps protect against server failure, making the data very fault-tolerant and durable. 
