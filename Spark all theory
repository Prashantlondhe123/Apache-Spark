#Apache spark :
Apache Spark is an open-source unified analytics engine for large-scale data processing. Spark provides an interface for programming clusters with implicit data parallelism and fault tolerance. Originally developed at the University of California, Berkeley's AMPLab, the Spark codebase was later donated to the Apache Software Foundation, which has maintained it since

Original author(s)   : Matei Zaharia
Developer(s)         : Apache Spark

Written in           : Scala[1]
Operating system     : Microsoft Windows, macOS, Linux
Available in         :Scala, Java, SQL, Python, R, C#, F#

#Apache Spark Features: 
In-memory computation
Distributed processing using parallelize
Can be used with many cluster managers (Spark, Yarn, Mesos e.t.c)
Fault-tolerant
Immutable
Lazy evaluation
Cache & persistence
Inbuild-optimization when using DataFrames
Supports ANSI SQL

#Apache Spark Advantages
Spark is a general-purpose, in-memory, fault-tolerant, distributed processing engine that allows you to process data efficiently in a distributed fashion.
Applications running on Spark are 100x faster than traditional systems.
You will get great benefits using Spark for data ingestion pipelines.
Using Spark we can process data from Hadoop HDFS, AWS S3, Databricks DBFS, Azure Blob Storage, and many file systems.
Spark also is used to process real-time data using Streaming and Kafka.
Using Spark Streaming you can also stream files from the file system and also stream from the socket.
Spark natively has machine learning and graph libraries.
Apache Spark Architecture
Apache Spark works in a master-slave architecture where the master is called “Driver” and slaves are called “Workers”. When you run a Spark application, Spark Driver creates a context that is an entry point to your application, and all operations (transformations and actions) are executed on worker nodes, and the resources are managed by Cluster Manager.
using StructType & StructField schema

#Cluster Manager Types:
Standalone –: a simple cluster manager included with Spark that makes it easy to set up a cluster.
Apache Mesos –: Mesons is a Cluster manager that can also run Hadoop MapReduce and Spark applications.
Hadoop YARN –: the resource manager in Hadoop 2. This is mostly used, cluster manager.
Kubernetes –: an open-source system for automating deployment, scaling, and management of containerized applications

#Spark Modules:
Spark Core
Spark SQL
Spark Streaming
Spark MLlib
Spark GraphX

#RDD Spark :
RDD (Resilient Distributed Dataset) is a fundamental data structure of Spark and it is the primary data abstraction in Apache Spark and the Spark Core. RDDs are fault-tolerant, immutable distributed collections of objects, which means once you create an RDD you cannot change it. Each dataset in RDD is divided into logical partitions, which can be computed on different nodes of the cluster

#Spark Streaming :
Spark Streaming is a scalable, high-throughput, fault-tolerant streaming processing system that supports both batch and streaming workloads. It is used to process real-time data from sources like file system folder, TCP socket, S3, Kafka, Flume, Twitter, and Amazon Kinesis to name a few. The processed data can be pushed to databases, Kafka, live dashboards e.t.c

#Structured Streaming:
Structured Streaming is a scalable and fault-tolerant stream processing engine built on the Spark SQL engine. You can express your streaming computation the same way you would express a batch computation on static data. The Spark SQL engine will take care of running it incrementally and continuously and updating the final result as streaming data continues to arrive. You can use the Dataset/DataFrame API in Scala, Java, Python or R to express streaming aggregations, event-time windows, stream-to-batch joins, etc. The computation is executed on the same optimized Spark SQL engine. Finally, the system ensures end-to-end exactly-once fault-tolerance guarantees through checkpointing and Write-Ahead Logs. In short, Structured Streaming provides fast, scalable, fault-tolerant, end-to-end exactly-once stream processing without the user having to reason about streaming

#Spark Streaming:
Spark Streaming is the previous generation of Spark’s streaming engine. There are no longer updates to Spark Streaming and it’s a legacy project. There is a newer and easier to use streaming engine in Spark called Structured Streaming. You should use Spark Structured Streaming for your streaming applications and pipelines. See Structured Streaming Programming Guide.

#Machine Learning Library (MLlib) :
MLlib is Spark’s machine learning (ML) library. Its goal is to make practical machine learning scalable and easy. At a high level, it provides tools such as:

ML Algorithms: common learning algorithms such as classification, regression, clustering, and collaborative filtering
Featurization: feature extraction, transformation, dimensionality reduction, and selection
Pipelines: tools for constructing, evaluating, and tuning ML Pipelines
Persistence: saving and load algorithms, models, and Pipelines
Utilities: linear algebra, statistics, data handling, etc

#Graphx:
GraphX is a new component in Spark for graphs and graph-parallel computation. At a high level, GraphX extends the Spark RDD by introducing a new Graph abstraction: a directed multigraph with properties attached to each vertex and edge. To support graph computation, GraphX exposes a set of fundamental operators (e.g., subgraph, joinVertices, and aggregateMessages) as well as an optimized variant of the Pregel API. In addition, GraphX includes a growing collection of graph algorithms and builders to simplify graph analytics tasks

#sparkR:
SparkR is an R package that provides a light-weight frontend to use Apache Spark from R. In Spark 3.3.0, SparkR provides a distributed data frame implementation that supports operations like selection, filtering, aggregation etc. (similar to R data frames, dplyr) but on large datasets. SparkR also supports distributed machine learning using MLlib

# Spark SQL CLI :
The Spark SQL CLI is a convenient interactive command tool to run the Hive metastore service and execute SQL queries input from the command line. Note that the Spark SQL CLI cannot talk to the Thrift JDBC server

#####################################################Pyspark#############

#Pyspark:
PySpark is a Spark library written in Python to run Python application using Apache Spark capabilities, using PySpark we can run applications parallelly on the distributed cluster (multiple nodes)

#########################################################spark streaming ###########################
Spark Streaming:
Kafka,flume ,HDFS/s3,kinesis , Twitter >spark Streaming >databases,HDFS, Dashboard

#Discretized Streams (DStreams)
Discretized Stream is the basic abstraction provided by Spark Streaming. It represents a continuous stream of data, either the input data stream received from the source or the processed data stream generated by transforming the input stream.

#Transformations on DStreams Transformation

Meaning

map(func)    :Return a new DStream by passing each element of the source DStream through a function func

flatMap(func)    :Similar to map, but each input item can be mapped to O or more output items

filter(func)    :Return a new DStream by selecting only the records of the source DStream on which func returns true

union(otherStream)  :Return a new DStream that contains the union of the elements in the source DStream and otherDStream

transform(func)  :Return a new DStream that contains the union of the elements in the source DStream and otherDStream

count()         :Return a new DStream of single-element RDDs by counting the number of elements in each RDD of the source DStream

join(otherStream, [numTasks]) :When called on two DStreams of (K, V) and (K. W) pairs, return a new DStream of (K. (V. W)) pairs with all pairs of elements for each key

csimple, All rights reserved

#Windowed Stream Processing:
Spark Streaming allows you to apply transformations over a sliding window of data. This operation is called as windowed computation

#Caching/Persistence:
DStreams allow developers to persist the stream's data in memory

With the help of persist() method. DStream will automatically persist every RDD of that DStream in memory


##############################Explaining the mechanics of Spark caching

#Caching strategies
There are several levels of data persistence in Apache Spark

1)MEMORY_ONLY:  Data is cached in memory in unserialized format only
2)MEMORY_AND_DISK. : Data is cached in memory. If memory is insufficient, the evicted blocks from memory are serialized to disk. This mode is recommended when re-evaluation is expensive and memory resources are limited.
3)DISK_ONLY. : Data is cached on disk only in serialized format
4)OFF_HEAP : Blocks are cached off-heap.

#Serialization: Serialization increases the cost of processing but reduces the amount of memory occupied by large data sets.
#Replication: Replication is useful for speeding up recovery in the event of a single cluster node (or executor) failure.

>>> df = spark.read.parquet(data_path)
>>> df.cache() # Cache the data
>>> df.count() # Materialize the cache, command took 5.11 seconds
>>> df.is_cached # Determining whether a dataframe is cached
True
>>> df.count() # Now get it from the cache, command took 0.44 seconds
>>> df.storageLevel # Determing the persistent type (memory, deserialized, # replicas)
StorageLevel(True, True, False, True, 1) 
>>> df.unpersist()

#
