#Spark optimization techniques:

Spark Job Optimisation Techniques.

1. Salting Techniques
2. Use Broadcast
3. Persist and cache
4. repartition and coalesce
5. Filter Pushdown
6. Avoid broad or wide transformation.
7. Avoid using "InferSchema."
8]  Bucketing:
9] Partition by
10] snappy & Gzip() compression Method 
11] Data skew:


1] Salting to Increase Parallelism: If there aren't enough distinct keys to create partitions, try salting to create them.

2] Use Broadcast: If there is a join between a small and a large table/dataframe, always try to broadcast the smaller one. This will avoid data shuffling since the broadcast dataframe/table will be available on the nodes locally.
-It is programming mechanism in Spark, through which we can keep read-only copy of data into each node of the cluster instead of sending it to node every time a task needs it.

>from pyspark.sql.functions import broadcast
>joinDF=transactionDF.join(broadcast(stoneDF), transactionDF ["Store_id"] == storeDFI'Store_id'])

3] Cache and Persist: Using cache() and persist() methods, Spark provides an optimisation mechanism to-store the intermediate computation of an RDD, DataFrame, and dataset so they can be reused in subsequent actions (reusing the RDD, Dataframe, and dataset computation result's).

4] repartition and coalesce: repartition() is used to increase or decrease the RDD, DataFrame, Dataset partitions whereas the coalesce() is used to only decrease the number of partitions in an efficient way.

5] Filter Pushdown : Always try to filter data as soon as possible before joining or aggregating. In the case of a dataframe or dataset, it will be taken care of by the catalyst optimiser.

6] Avoid Wide Transformation: Wherever possible, always try to avoid wide transformation since it does shuffle and takes a lot of time.

7] Avoid InferSchema : Avoid InferSchema whenever possible when reading data from a file, because it will take a long time to retrieve metadata (schema) information from the file. Always create a custom schema in spark using a struct and use it in a dataframe.

8]  Bucketing:
Bucketing is an optimization technique in Spark SQL that uses buckets and bucketing columns to determine data partitioning. When applied properly bucketing can lead to join optimizations by avoiding shuffles (aka exchanges) of tables participating in the join. The talk will give you the necessary information so you can use bucketing to optimize Spark SQL structured queries

-Create Bucketed Table :PK is a primary key col 
>df.write.format("parquet").bucketBy(, "PK").saveAsTable("bucketedTable")

-Created Bucketed  Dataframe:
>df1 spark.table("bucketedTable")
>df2 = spark.table("bucketedTable")


9] partition by :
df.write.option("header", True).partitionBy("Year").mode("overwrite").csv("/FileStore/tables/baby_names_output")
df.write.option("header",True).partitionBy("Year","Sex").mode("overwrite").csv ("/FileStore/tables/baby_names_output")
df.write.option("header",True).option("maxRecordsPerFile",4200).partitionBy("Year").mode("overwrite").csv("/FileStore/tables/baby_names_output")

10]#snappy:
-Low CPU Utilization
-Low Compression Rate
-Splitable
-Use Case: Hot Layer
-Use Case: Compute Intensive

>csVDF.write.format("parquet").option("compression", "snappy").save("/FileStore/tables/bigdata_training/write_files/snappy_parquets")

#Gzip():
-High CPU Utilization
-High Compression Rate
-Non-splitable
-Use Case: Cold Layer
-Use Case: Storage Intensive

> csvDF.write.format("parquet").option("compression", "gzip").save("/FileStore/tables/bigdata_training/write_files/gzip_parquets")

11]Data Skew:
-Data skew is a condition in which a table's data is unevenly distributed among partitions in the cluster.
-Data skew can severely downgrade performance of queries, especially those with joins.
-Joins between big tables require shuffling data and the skew can lead to an extreme imbalance of work in the cluster.
-It's likely that data skew is affecting a query if a query appears to be stuck finishing very few tasks (for example, the last 3 tasks out of

>How to handle Data Skew:
- Salt the skewed column with a random number creating better distribution across each partition
-Apply Skew-hint, With the information from these
hints, Spark can construct a better query plan, one that does not suffer from data skew.
-Use Broadcast Join for smaller tables
-Enable Adaptive query execution if you are using Spark 3 which will balance out the partitions for us automatically

12]
