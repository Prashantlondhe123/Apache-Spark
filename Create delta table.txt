Delta table : 
-Delta Lake is an open format storage layer that delivers reliability, security and performance on your data lake - for both streaming and batch operations.
-By replacing data silos with a single home for structured, semi-structured and unstructured data, Delta Lake is the foundation of a cost-effective, highly scalable Lakehouse.
- Delta lake for acid transaction/dml operation
- delta follow soft delete. Means after deletion delta Lake keep data in a table for 7 days. This is default time . We can also set time .

#Delta Lake Solution Architecture:
      1]Raw Data Integration.    2] Cleansed + Transformed      3]Business Aggregated
Adls >>      Bronze.                     Silver.                      Gold.                >> Power Bi, excel 
            Data Lake.                  Delta Lake                   Delta lake





## Create delta table  Method 1 :

from delta.tables import *

DeltaTable.create(spark) .tableName("employee_demo").addColumn("emp_name", "STRING").addColumn("gender", "STRING").addColumn("salary", "INT")
.addColumn("Dept", "STRING").property("description", "table created for demo purpose") A
.location("/FileStore/tables/delta/createtable")
.execute()

>> Select * from t_name ;



# Method 2:
from delta.tables import *
DeltaTable.createIfExists(spark).tableName("employee, deno").addColumn("emp_id", "INT").addColumn("emp_name", "STRING").addColumn("gender", "STRING").addColumn("salary", "INT").addColumn("Dept", "STRING")
.execute()


#Method 3:
from delta.tables import *
DeltaTable.createOrReplace(spark).tableName("employee demo").addColumn("emp_id", "INT").addColumn("emp_name", "STRING").addColumn("gender", "STRING").addColumn("salary", "INT").addColumn("Dept", "STRING")
property("description", "table created for deno purpose")
.location("/FileStore/tables/delta/createtable") 
.execute()


## create Delta table with sql :
#Method 1:
CREATE TABLE endoyee deno (
emp_id INT,
emp Name STRING,
gender STRING,
salary INT,
dept STRING,
) USING DELTA


# Method 2:
CREATE TABLE IF NOT EXISTS employee demo (
emp, id INT,
emp Name STRING,
gender STRING,
salary INT,
dept STRING,
) USING DELTA


# Method 3:
CREATE OR REPLACE TABLE employee demo (
empid INT,
emp Name STRING,
gender STRING,
salary INT,
dept STRING,
)USING DELTA
LOCATION/FileStore/tables/delta/createtable"



## create Delta table with dataframe  :
df.write.format("delta").saveAsTable("employee_demo1")

>>select * from employee_demol
################insert new record  in a table #####
>insert into table_name values (1,2,3)


##############delete delta table #####

>delete from table_name where condition


###########update delta table #####
>Update  table_name set col ="" where condition 
>Update delta /file/ set col="" where condition 

#Pyspark:
>from delta.tables import *
from pyspark.sql.functions import.
deltaTable DeltaTable.forPath(spark, "/FileStore/tables/delta/path employee demo")


deltaTable.update(
condition: "emp_name ="Mark",
 set{"salary": "15000"}
 )

################Delta optimization ####
#check history of  delta table:
>describe history table_name

#It creates one log file . removs  all  previous log files
 >Optimize table_name


# it will list all file 
>Vacuum table_name1 DRY RUN

#it will list all file which should be deleted : time:0 hr old file will delete 
>Vacuum table name retain 0 hours dry run 

# delete log file if  file is created before 7 days
> Vacuum table_name

# Z order: it is used to improve performance. It combine all file according to one column. With the help of that column we can fetch data easily or fastly.
>Vacuum table_name ZORDER c1



