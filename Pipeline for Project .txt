##pipeline for project:

#Step1:
%run/Shared/CAMPAIGN TEAM_JOBS/GLOBAL_JOBS/COMMON/CONFIGURATION

#Step 2:
from pyspark.sql. functions import *
from pyspark.sql. functions import col

#Step3:CREATING WIDGETS
try:

     dbutils.widgets.text('MIS_DATE','')
     dbutils.widgets.text('ENVIRONMENT','')
     #FETCHING VALUES IN WIDGETS AND PUT THIS VALUES IN A VARIABLE
      MIS_DATE = dbutils.widgets.get('MIS_DATE")
      ENVIRONMENT = dbutils.widgets.get('ENVIRONMENT')

except Exception as e:
      print(e)


#Steps4:#CONFIGURING CONNECTION FOR ADLS
try:
    OBJECT CONFIG_SET()
    SET OBJECT.return_config(f' {ENVIRONMENT}','craindevelopmentzone')
    spark.conf.set("fs.azure.sas.craindevelopmentzone.dldevadls.blob.core.windows.net",SET SASTOKEN]

     #CONFIGURING CONNECTION FOR SQL SERVER
     OBJECT = CONFIG_SET()
     sql_server_config=CONFIG_SET().return_config(f' (ENVIRONMENT}", "sql_server")

except Exception as e:
     print(e)


#Steps5: Reading data from a adls
Read_main_Files (1: IOT,2: INQUIRY,3: GROUPACCOUNT)
dataframe list = []
for i,j in Read_main_Files.items():
    try:
        file_read = spark.read.csv(f"wasbs://(CONTAINER NAME)@didevadis.blob.core.windows.net/TRANSACTIONAL DATA/Sample Sink/CHF NEW/(1)/+header ,inferSchema-True)
        df=replaceExtra(file_read)
        dataframe list.append(df)
        df.createDrReplaceTempView(f" (j) View")

     except Exception as e:
        print(e)
